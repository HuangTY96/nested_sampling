%
% svn info 
% $Date$
% $Rev$
%
\documentclass[a4paper]{article}

%\usepackage[numbers]{natbib}
\usepackage[top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{amsmath}
\bibliographystyle{naturemagurl}
%\bibliographystyle{plainnat}


\title{Notes on coupling nested sampling with basin hopping}
\author{Jacob Stevenson and Stefano Martiniani}
\date{\today}

\begin{document}
\maketitle


\section{Nested Sampling}

The nested sampling algorithm is:
\begin{enumerate}
  \item Start by initializing $N_r$ replicas.  These should be completely random.  Some will have extremely high 
    temperatures, but that is fine.

  \item Remove the replica with the highest energy from your list and save the energy as $E_{max}$

  \item Get a configuration, $x$, sampled uniformly in configuration space
    with the only criterion that $E(x) \le E_{max}$.  Note that this is not trivial and will
    be discussed further below
  
  \item Add $x$ to the collection of replicas

  \item goto 2.

  \item the density of states and all other thermodynamic parameters can be calculated easily from the list of energies $E_{max}$

\end{enumerate}


The second step, ``sample $x$ uniformly in configuration space such that $E(x) < E_{max}$'' is
impossible to do exactly, so we explore phase space iterative for a given
number of steps and let $x$ be the stopping configuration.
It is generally done as follows:

\begin{enumerate}
  \item select a replica $x_r$ randomly from our collection of $N_r - 1$ replicas
  
  \item use $x_r$ as a starting point for a monte carlo chain.  The acceptance condition 
    of the monte carlo chain is simply: accept if $E_{new} < E_{max}$.  
\end{enumerate}

Note that this is not thermodynamic sampling.  We want to sample in an unbiased
way from all of configuration space with $E(x) < E_{max}$


\section{Sampling from the harmonic superposition approximation}

A weakness of the nested sampling algorithm is one of sampling.  Imagine the following situation
of trying to describe a crystallization transition.  At high energies the liquid state
completely dominates phase spase.  Nearly all of the replicas will be in the disordered liquid
configurations.  As $E_{max}$ decreases the relative weight of the crystal phse becomes more 
and more important, and you would start to expect to see some replicas in the crystal.  This is
your standard sampling problem that plagues molecular dynamics and monte carlo.
It can be really really difficult to find the crystal.  If you end up in a
situation where no replicas are in the crystal phase your results will then
completely miss the transition.  It is even worse for nested sampling because $E_{max}$ decreases
monotonically. For $E_{max}$ very high it's unlikely to be in the crystal because the phase space 
volume is so small.  For $E_{max}$ very low the relative phase volume is very
favorable, but exploring the energy landscape is very restricted so it is very
unlikely, or even impossible to find it starting from a disordered
configuration.  There is a relatively small range of $E_{max}$ where enough stars align so that
it is likely that the crystal is found.

We will use Basinhopping to calculate the ground state and all relevant low lying minima.
We will then use this information to improve the step of the nested sampling
algorithm ``sample uniformly from configuration space with $E(x) < E_{max}$''.

In the harmonic approximation the density of states of minimum $\alpha$ is

\begin{equation}
  \Omega(E) = \frac{2N!(E - E_\alpha)^{k-1} }{\Gamma(k) \bar{\nu}^k O_\alpha}
\end{equation}

$E_\alpha$ is the energy of the minimum.  $\Gamma(k)$ is the gamma function $(k-1)!$, $k$ is the number
of vibrational degrees of freedom ($3*N-6$ for a cluster).  $\bar{\nu}$ is the
geometric mean of the positive normal mode frequencies, or, put another way,
$\bar{\nu}^k$ is the product of the positive normal mode frequencies
$\nu_{\alpha}$.  $2N!$ accounts for all the non-superimposible
permutation-inversion isomers of each structure. $O_\alpha$ is the order of the
symmetry point group of the structure of minimum $\alpha$.  The $2N!$
permutations and the gamma function are the same for all structures and will
cancel out when renormalising.
To get the phase space volume of the basin of a minimum we integrate the energy

\begin{equation}
  V_{\alpha}(E_{max}) = \int_{E_{\alpha}}^{E_{max}} \Omega(E) dE = \frac{2N!(E_{max} - E_\alpha)^{k} }{\Gamma(k+1) \bar{\nu}^k O_\alpha}
\end{equation}

To sample uniformly from the harmonic approximation we

\begin{enumerate}
  \item select a minimum $\alpha$ uniformly with weight $V_{\alpha} / V_{tot}$

  \item sample uniformly from with basin $\alpha$
\end{enumerate}

\subsection{Sample uniformly in a basin}

There are several methods of doing this.  We can use $x_{\alpha}$ as a starting
point for our monte carlo chain as described above.  We can displace using the known
eigenvalues and eigenvectors from the minimum.  Or we can do a combination.  Lets
first talk about how to use the eigenvalues $\mu_i$ and orthonormal eigenvectors $\vec{v_i}$ 

\begin{equation}
  \vec{x} = \vec{x}_{\alpha} + \sum_{i=1}^k q_i (\vec{v_i} / \sqrt{\mu_i})
  \label{eqn:step_away}
\end{equation}

We scale the vectors by $\vec{v_i} / \sqrt{\mu_i}$ so that all directions have
the same curvature.  The problem is now reduced to how to choose the $q_i$ such
that we are sampling uniformly in configuration space.  Scaling the vectors
above make all directions uniform, so the unit vector $\vec{q} / ||\vec{q}||$ should be 
uniform. (Note: sampling a unit vector uniformly is done by choosing all components from
a normal distribution and normalizing the resulting vector)

We are now left with determining the appropriate magnitude of the vector $||\vec{q}|| = q$.  Stepping in any one direction with a stepsize of $q$ 

\begin{equation}
  x = x_{\alpha} + q (\vec{v_i} / \sqrt{\mu_i})
\end{equation}
results in an energy change of
\begin{equation}
  E = E_{\alpha} + \frac{1}{2} q^2
  \label{eqn:echange}
\end{equation}

The total magnitude of the step defined in \ref{eqn:step_away} is $q =
\left( \sum{q_i^2} \right)^{\frac{1}{2}} = \left|\left|\vec{q}\right|\right|$ which results in a total energy change given, in
exactly the same way as \ref{eqn:echange}.  So $q$ can be at maximum

\begin{equation}
  q_{max} = \sqrt{2 (E_{max} - E_{\alpha})}
\end{equation}


But from what distribution should $q$ be drawn?  
We know that the density of states grows like $(E-E_{\alpha})^{k-1}$.
Thus the probability of drawing a configuration with energy
between $E$ and $E+dE$ should be 
\begin{equation}
  P(E)dE \propto (E-E_{\alpha})^{k-2}dE
\end{equation}
In $q$ space this probability density function becomes $P(q)dq = P(E)dE$
\begin{equation}
  P(q)dq \propto q^{2k-3}dq
\end{equation}

Thus we draw $q$ from a power law distribution with power $(2k-2)$ and maximum
value $\sqrt{2 (E_{max} - E_{\alpha})}$.  Note that the probability density
function above with power $k-1$ has a cumulative distribution function with
power $k$, so these distributions are reffered to as power law distributions
with power $k$.
Alternatively we can draw the target change in energy $dE_{target}$ from a power law
distribution with power $k-1$ and maximum value $(E_{max} - E_{\alpha})$
and set $q = \sqrt{2 dE_{target}}$.


\section{parallel nested sampling}

Nested sampling can be parallelized to run on P cores very easily.
At every step we 

\begin{enumerate}
  \item remove the replica with the highest energy, and save that energy for post-analysis

  \item remove also the next P-1 highest energy replicas.

  \item Select P replicas at random from those remaining and use those as a starting
    configuration for a Monte Carlo run

  \item Insert the P new configurations into the list of replicas
\end{enumerate}

The improvement of this over the serial version is that each replica spends more time 
evolving via the monte carlo chain, and so the phase space is explored more completely.

It would be nice to know how many monte carlo iterations each replica on
average takes.  Or, phrased another way, at iteration $i$, what is the average
number of MC steps a given replica has taken. If $\bar{n}_i$ is the average number
of monte carlo steps at iteration $i$, we can work out what the average number is
at iteration $i+1$.  

\begin{itemize}
  \item $P$ replicas are thrown away, but they are selected in a way independent of the
    number of MC steps.  So the average number of MC steps remains $\bar{n}_i$
    
  \item from the remaining replicas $P$ are chosen at random.  The average of these
    replicas is again $\bar{n}_i$.

  \item the chosen replicas are duplicated and run for an additional $n_{mc}$ MC steps
\end{itemize}

Thus we can write down the new average value of the number of MC steps
\begin{equation}
  \bar{n}_{i+1} K = \bar{n}_i  (K - P) + (\bar{n}_i + n_{mc}) P
\end{equation}
\begin{equation}
  \bar{n}_{i+1} = \bar{n}_i  +  n_{mc} P / K
  \label{eqn:par_iter}
\end{equation}
From this iterative relationship we can work out the average number of
MC steps at step $i$
\begin{equation}
  \bar{n}_{i} = i n_{mc} P / K
\end{equation}
This is linear in P and grows linearly with the number of MC steps.

Note that each one of these iterations is at a different, $E_{max}$, so this
not equivalent to simply doing longer MC runs. Also note that if $P$ is 1 then
we still get a total number of MC steps increasing linearly with the number of
iterations

\subsection{sampling from the minima}

If we include sampling from the minima we change the above calculation of the
average number of MC steps.  Each time we sample from the minima we reset
one of the replicas to have 0 MC steps.  Lets go through the above
arguments again. $P_{m}$ is the average number of the $P$ new replicas starting
from zero

\begin{itemize}
  \item $P$ replicas are thrown away, but they are selected in a way independent of the
    number of MC steps.  So the average number of MC steps remains $\bar{n}_i$

  \item $P_m$ replicas are started from one of the minima
    
  \item from the remaining replicas $P - P_m$ are chosen at random.  The average of these
    replicas is again $\bar{n}_i$.  

  \item the chosen replicas are duplicated and run for an additional $n_{mc}$ MC steps
\end{itemize}
The equations are altered as
\begin{equation}
  \bar{n}_{i+1} K = \bar{n}_i  (K - P) + (\bar{n}_i + n_{mc}) (P - P_m) + n_{mc} P_m
\end{equation}
\begin{equation}
  \bar{n}_{i+1} = \bar{n}_i (1-P_m/K)  +  n_{mc} P / K
\end{equation}

In contrast to equation \label{eqn:par_iter}, the multiplicative factor $(1-P_m/K)$
means that large values of  $\bar{n}_i$ will decay down to a steady state.  That 
steady state can be worked out by setting $\bar{n}_{i+1} = \bar{n}_i$
\begin{equation}
  \bar{n}_{i}^{ss} = n_{mc} P / P_m
\end{equation}
Thus, when we sample from the minima, the average number of monte carlo steps saturates
after many iterations.

\bibliography{jakes_biblio}

\end{document}
